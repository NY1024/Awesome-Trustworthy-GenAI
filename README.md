# Awesome-Trustworthy-GenAI [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
<a href=""> <img src="https://img.shields.io/github/stars/NY1024/Awesome-Trustworthy-GenAI?style=flat-square&logo=github" alt="GitHub stars"></a>
<a href=""> <img src="https://img.shields.io/github/forks/NY1024/Awesome-Trustworthy-GenAI?style=flat-square&logo=github" alt="GitHub forks"></a>
<a href=""> <img src="https://img.shields.io/github/last-commit/NY1024/Awesome-Trustworthy-GenAI"></a>
</p>

---
Update[06/29/2024]
### Book
| Title                                                        | Author                                                       | Publish        | Year | Link                                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ | -------------- | ---- | ----------------------------------------------------------- |
| Adversarial Machine Learning A Taxonomy and Terminology of Attacks and Mitigations | Apostol Vassilev                                             | online         | 2024 | https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf |
| 人工智能安全                                                 | 方滨兴                                                       | 电子工业出版社 | 2022 | null                                                        |
| 人工智能安全                                                 | 曾剑平                                                       | 清华大学出版社 | 2022 | null                                                        |
| 人工智能安全                                                 | 陈左宁                                                       | 电子工业出版社 | 2024 | null                                                        |
| AI安全:技术与实战                                            | 腾讯安全朱雀实验室 作者：流浪在银河边缘的阿强 https://www.bilibili.com/read/cv24788984/ 出处：bilibili | 电子工业出版社 | 2022 | null                                                        |
| Trustworthy Machine Learning                                 | Kush R. Varshney                                             | null           | 2022 | null                                                        |
| 人工智能:数据与模型安全                                      | 姜育刚                                                       | 机械工业出版社 | 2024 | null                                                        |


### Attack

| Title                                                        | Author          | Publish | Year | Link                                                         | Source Code                                                  |
| ------------------------------------------------------------ | --------------- | ------- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Visual Adversarial Examples Jailbreak Large Language Models  | Xiangyu Qi      | AAAI    | 2024 | [AAAI](https://ojs.aaai.org/index.php/AAAI/article/view/30150/32038) | [github](https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models) |
| Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak Attacks | Zonghao Ying    | arxiv   | 2024 | https://arxiv.org/abs/2406.06302                             | https://github.com/ny1024/jailbreak_gpt4o                    |
| Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt | Zonghao Ying    | arxiv   | 2024 | https://arxiv.org/abs/2406.04031                             | https://github.com/NY1024/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt |
| Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models | Erfan Shayegani | ICLR    | 2024 | https://openreview.net/forum?id=plmBsXHxgR                   | null                                                         |
| FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts | Yichen Gong     | arxiv   | 2023 | https://arxiv.org/abs/2311.05608                             | https://github.com/ThuCCSLab/FigStep                         |
| Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models | Yifan Li        | arxiv   | 2024 | https://arxiv.org/abs/2403.09792                             | https://github.com/AoiDragon/HADES                           |
| Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks? | Shuo Chen       | arxiv   | 2024 | https://arxiv.org/abs/2404.03411                             | null                                                         |
| Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models | Xijie Huang     | arxiv   | 2024 | https://arxiv.org/abs/2405.20775                             | https://github.com/dirtycomputer/O2M_attack                  |
| Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character | Siyuan Ma       | arxiv   | 2024 | https://arxiv.org/abs/2405.20773                             | null                                                         |
---
## Categories
- [Paper](paper.md)
  - [LLM](paper/llm.md)
  - [MLLM](paper/mllm.md)
  - [T2I](paper/t2i.md)
  - [Agent](paper/agent.md)
- [Book](book.md)
- [Tutorial](tutorial.md)
- [Competition](competition.md)



---

[![Star History Chart](https://api.star-history.com/svg?repos=NY1024/Awesome-Trustworthy-GenAI&type=Date)](https://star-history.com/#NY1024/Awesome-Trustworthy-GenAI&Date)

---
- Organizers: [Zonghao Ying(应宗浩)](https://elwood.me/)

<p align="center"><img src="figs/1.png"/></p>
