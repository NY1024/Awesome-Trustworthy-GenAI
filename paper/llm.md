# LLM

## Jailbreak

### Attack

| Title                                                        | Author                | Publish                                   | Date     | Link                                     | Source Code                                                  |
| ------------------------------------------------------------ | --------------------- | ----------------------------------------- | -------- | ---------------------------------------- | ------------------------------------------------------------ |
| Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions | Federico Bianchi      | ICLR Poster                               | Mar-24   | https://openreview.net/pdf?id=gT5hALch9z | null                                                         |
| FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models | Dongyu Yao            | ICASSP                                    | Sep-23   | https://arxiv.org/abs/2309.05274         | https://arxiv.org/pdf/2309.05274                             |
| GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts | Jiahao Yu             | arxiv                                     | Sep-23   | https://arxiv.org/abs/2309.10253         | https://github.com/sherdencooper/GPTFuzz                     |
| Open Sesame! Universal Black Box Jailbreaking of Large Language Models | Raz Lapid             | arxiv                                     | Sep-23   | https://arxiv.org/abs/2309.01446         | null                                                         |
| Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment | Rishabh Bhardwaj      | arxiv                                     | Aug-23   | https://arxiv.org/abs/2308.09662         | null                                                         |
| Jailbroken: How Does LLM Safety Training Fail?               | Alexander Wei         | NeurIPS                                   | Jul-23   | https://arxiv.org/abs/2307.02483         | null                                                         |
| MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots | Gelei Deng            | NDSS                                      | Jul-23   | https://arxiv.org/abs/2307.08715         | null                                                         |
| Universal and Transferable Adversarial Attacks on Aligned Language Models | Andy Zou              | NeurIPS                                   | Jul 2023 | https://arxiv.org/abs/2307.15043         | http://llm-attacks.org/                                      |
| Adversarial Demonstration Attacks on Large Language Models   | Jiongxiao Wang        | arxiv                                     | May-23   | https://arxiv.org/abs/2305.14950         | null                                                         |
| Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation | Yangsibo Huang        | ICLR                                      | Mar-24   | https://openreview.net/pdf?id=r42tSSCHPh | https://github.com/Princeton-SysML/Jailbreak_LLM             |
| GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher | Youliang Yuan         |                                           | Jan-24   | https://openreview.net/pdf?id=MbfAK4s61A | https://github.com/RobustNLP/CipherChat                      |
| AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models | Xiaogeng Liu          | ICLR                                      | Jan-24   | https://openreview.net/pdf?id=7Jwpw4qKkb | https://github.com/SheltonLiu-N/AutoDAN                      |
| Low-Resource Languages Jailbreak GPT-4                       | Zheng-Xin Yong        | NeurIPS Workshop                          | Oct-23   | https://arxiv.org/abs/2310.02446         | null                                                         |
| Multi-step Jailbreaking Privacy Attacks on ChatGPT           | Haoran Li             | EMNLP                                     | Apr-23   | https://arxiv.org/abs/2304.05197         | null                                                         |
| Attack Prompt Generation for Red Teaming and Defending Large Language Models | Boyi Deng             | EMNLP                                     | Oct-23   | https://arxiv.org/abs/2310.12505         | https://github.com/Aatrox103/SAP                             |
| Multilingual Jailbreak Challenges in Large Language Models   | Yue Deng              | ICLR                                      | Oct-23   | https://arxiv.org/abs/2310.06474         | https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs%7D |
| Weak-to-Strong Jailbreaking on Large Language Models         | Xuandong Zhao         | arxiv                                     | Jan-24   | https://arxiv.org/abs/2401.17256         | https://github.com/XuandongZhao/weak-to-strong               |
| How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs | Yi Zeng               | ACL                                       | Jan-24   | https://arxiv.org/abs/2401.06373         | https://chats-lab.github.io/persuasive_jailbreaker/          |
| Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts | Yuanwei Wu            | arxiv                                     | Nov-23   | https://arxiv.org/abs/2311.09127         | null                                                         |
| Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack | Yu Fu                 | ACL                                       | Dec-23   | https://arxiv.org/abs/2312.06924         | null                                                         |
| Tree of Attacks: Jailbreaking Black-Box LLMs Automatically   | Anay Mehrotra         | arxiv                                     | Dec-23   | https://arxiv.org/abs/2312.02119         | https://github.com/RICommunity/TAP                           |
| A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily | Peng Ding             | NAACL                                     | Nov-23   | https://arxiv.org/abs/2311.08268         | https://github.com/NJUNLP/ReNeLLM                            |
| Goal-Oriented Prompt Attack and Safety Evaluation for LLMs   | Chengyuan Liu         | arxiv                                     | Sep-23   | https://arxiv.org/abs/2309.11830         | https://github.com/liuchengyuan123/CPAD                      |
| Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues | Zhiyuan Chang         | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.09091         | null                                                         |
| A Cross-Language Investigation into Jailbreak Attacks in Large Language Models | Jie Li                | arxiv                                     | Jan-24   | https://arxiv.org/abs/2401.16765         | null                                                         |
| Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak | Yanrui Du             | arxiv                                     | Dec-23   | https://arxiv.org/abs/2312.04127         | null                                                         |
| All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks | Kazuhiro Takemoto     | arxiv                                     | Jan 2024 | https://arxiv.org/abs/2401.09798         | null                                                         |
| DeepInception: Hypnotize Large Language Model to Be Jailbreaker | Xuan Li               | arxiv                                     | Nov-23   | https://arxiv.org/abs/2311.03191         | https://github.com/tmlr-group/DeepInception                  |
| Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation | Rusheb Shah           | arxiv                                     | Nov-23   | https://arxiv.org/abs/2311.03348         | null                                                         |
| PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails | Neal Mangaokar        | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.15911         | null                                                         |
| Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs | Xiaoxia Li            | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.14872         | null                                                         |
| ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings | Hao Wang              | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.16006         | null                                                         |
| A StrongREJECT for Empty Jailbreaks                          | Alexandra Souly       | [arxiv](https://arxiv.org/abs/2402.10260) | Feb-24   | https://arxiv.org/abs/2402.10260         | https://github.com/alexandrasouly/strongreject               |
| Jailbreaking Black Box Large Language Models in Twenty Queries | Patrick Chao          | arxiv                                     | Oct-23   | https://arxiv.org/abs/2310.08419         | https://github.com/patrickrchao/JailbreakingLLMs             |
| Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations | Zeming Wei            | arxiv                                     | Oct-23   | https://arxiv.org/abs/2310.06387         | null                                                         |
| AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models | Sicheng Zhu           | arxiv                                     | Oct-23   | https://arxiv.org/abs/2310.15140         | null                                                         |
| Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks | Daniel Kang           | arxiv                                     | Feb 2023 | https://arxiv.org/abs/2302.05733         | null                                                         |
| Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning | Gelei Deng            | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.08416         | null                                                         |
| Jailbreaking Proprietary Large Language Models using Word Substitution Cipher | Divij Handa           | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.10601         | null                                                         |
| PAL: Proxy-Guided Black-Box Attack on Large Language Models  | Chawin Sitawarin      | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.09674         | https://github.com/chawins/pal                               |
| ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs | Fengqing Jiang        | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.11753         | https://github.com/uw-nsl/ArtPrompt                          |
| Query-Based Adversarial Prompt Generation                    | Jonathan Hayase       | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.12329         | null                                                         |
| Coercing LLMs to do and reveal (almost) anything             | Jonas Geiping         | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.14020         | https://github.com/JonasGeiping/carving                      |
| COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability | Xingang Guo           | ICML                                      | Feb-24   | https://arxiv.org/abs/2402.08679         | https://github.com/Yu-Fangxu/COLD-Attack                     |
| Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks | Maksym Andriushchenko | arxiv                                     | Apr-24   | https://arxiv.org/abs/2404.02151         | https://github.com/tml-epfl/llm-adaptive-attacks             |
| Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs | Bibek Upadhayay       | arxiv                                     | Apr 2024 | https://arxiv.org/abs/2404.07242         | null                                                         |
| Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models | Zhiyuan Yu            | USENIX Security                           | Mar-24   | https://arxiv.org/abs/2403.17336         | null                                                         |
| CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion | Qibing Ren            | ACL                                       | Mar-24   | https://arxiv.org/abs/2403.07865         | https://github.com/renqibing/CodeAttack                      |
| Tastle: Distract Large Language Models for Automatic Jailbreak Attack | Zeguan Xiao           | arxiv                                     | Mar-24   | https://arxiv.org/abs/2403.08424         | null                                                         |
| AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs | Zeyi Liao             | arxiv                                     | Apr-24   | https://arxiv.org/abs/2404.07921         | https://github.com/OSU-NLP-Group/AmpleGCG                    |
| Chain of Attack: a Semantic-Driven Contextual Multi-Turn attacker for LLM | Xikang Yang           | arxiv                                     | May-24   | https://arxiv.org/abs/2405.05610         | null                                                         |
| GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models | Haibo Jin             | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.03299         | null                                                         |
| Don't Say No: Jailbreaking LLM by Suppressing Refusal        | Yukai Zhou            | arxiv                                     | Apr-24   | https://arxiv.org/abs/2404.16369         | null                                                         |
| AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs    | Anselm Paulus         | arxiv                                     | Apr-24   | https://arxiv.org/abs/2404.16873         | https://github.com/facebookresearch/advprompter              |
| Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation | Yuxi Li               | arxiv                                     | May-24   | https://arxiv.org/abs/2405.13068         | null                                                         |
| Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent | Shang Shang           | arxiv                                     | May-24   | https://arxiv.org/abs/2405.03654         | null                                                         |
| GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation | Govind Ramesh         | arxiv                                     | May-24   | https://arxiv.org/abs/2405.13077         | null                                                         |
| Poisoned LangChain: Jailbreak LLMs by LangChain              | Ziqiu Wang            | arxiv                                     | Jun-24   | https://arxiv.org/abs/2406.18122         | https://github.com/CAM-FSS/jailbreak-langchain               |
| Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation | Danny Halawi          | ICML                                      | Jun-24   | https://arxiv.org/abs/2406.20053         | null                                                         |
| Voice Jailbreak Attacks Against GPT-4o                       | Xinyue Shen           | arxiv                                     | May-24   | https://arxiv.org/abs/2405.19103         | https://github.com/TrustAIRLab/VoiceJailbreakAttack          |
| StructuralSleight: Automated Jailbreak Attacks on Large Language Models Utilizing Uncommon Text-Encoded Structure | Bangxin Li            | arxiv                                     | Jun-24   | https://arxiv.org/abs/2406.08754         | null                                                         |
| Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack | Shangqing Tu          | arxiv                                     | Jun-24   | https://arxiv.org/abs/2406.11682         | https://github.com/THU-KEG/Knowledge-to-Jailbreak/           |
| CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models | Huijie Lv             | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.16717         | https://github.com/huizhang-L/CodeChameleon                  |
| DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers | Xirui Li              | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.16914         | https://github.com/xirui-li/DrAttack                         |
| Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction | Tong Liu              | USENIX Security                           | Feb-24   | https://arxiv.org/abs/2402.18104         | https://github.com/LLM-DRA/DRA                               |
| Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks | Yixin Cheng           | arxiv                                     | Feb-24   | https://arxiv.org/abs/2402.09177         | null                                                         |
| Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters | Haibo Jin             | arxiv                                     | May-24   | https://arxiv.org/abs/2405.20413         | null                                                         |
| RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs | Xuan Chen             | arxiv                                     | Jun-24   | https://arxiv.org/abs/2406.08725         | null                                                         |
| Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses | Xiaosen Zheng         | arxiv                                     | Jun-24   | https://arxiv.org/abs/2406.01288         | https://github.com/sail-sg/I-FSJ                             |
| AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens | Lin Lu                | arxiv                                     | Jun-24   | https://arxiv.org/abs/2406.03805         | null                                                         |
| Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack | Mark Russinovich      | arxiv                                     | Apr-24   | https://arxiv.org/abs/2404.01833         | null                                                         |
| MART: Improving LLM Safety with Multi-round Automatic Red-Teaming | Suyu Ge               | arxiv                                     | Nov-23   | https://arxiv.org/abs/2311.07689         | null                                                         |
| Virtual Context: Enhancing Jailbreak Attacks with Special Token Injection | Yuqi Zhou             | arxiv                                     | Jun 2024 | https://arxiv.org/abs/2406.19845         | null                                                         |
| When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search | Xuan Chen             | arxiv                                     | Jun-24   | https://arxiv.org/abs/2406.08705         | null                                                         |
| Improved Techniques for Optimization-Based Jailbreaking on Large Language Models | Xiaojun Jia           | arxiv                                     | May-24   | https://arxiv.org/abs/2405.21018         | https://github.com/jiaxiaojunQAQ/I-GCG                       |

### Defense

| Title                                                        | Author              | Publish     | Date   | Link                                     | Source Code                                                  |
| ------------------------------------------------------------ | ------------------- | ----------- | ------ | ---------------------------------------- | ------------------------------------------------------------ |
| Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF | Anand Siththaranjan | ICLR        | Apr-24 | https://openreview.net/pdf?id=0tWTxYYPnW | null                                                         |
| Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM | Bochuan Cao         | ACL         | 2024   | https://arxiv.org/abs/2309.14348         | null                                                         |
| Detecting Language Model Attacks with Perplexity             | Gabriel Alon        | arxiv       | Aug-23 | https://arxiv.org/abs/2308.14132         | null                                                         |
| Certifying LLM Safety against Adversarial Prompting          | Aounon Kumar        | arxiv       | Sep-23 | https://arxiv.org/abs/2309.02705         | https://github.com/aounon/certified-llm-safety               |
| Baseline Defenses for Adversarial Attacks Against Aligned Language Models | Neel Jain           | arxiv       | Sep-23 | https://arxiv.org/abs/2309.00614         | null                                                         |
| SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks | Alexander Robey     | arxiv       | Oct-23 | https://arxiv.org/abs/2310.03684         | https://github.com/arobey1/smooth-llm                        |
| Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks | Abhinav Rao         | LREC-COLING | May-23 | https://arxiv.org/abs/2305.14965         | null                                                         |
| Intention Analysis Makes LLMs A Good Jailbreak Defender      | Yuqi Zhang          | arxiv       | Jan-24 | https://arxiv.org/abs/2401.06561         | https://github.com/alphadl/SafeLLM_with_IntentionAnalysis    |
| On Prompt-Driven Safeguarding for Large Language Models      | Chujie Zheng        | ICML        | Jan-24 | https://arxiv.org/abs/2401.18018         | https://github.com/chujiezheng/LLM-Safeguard                 |
| Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks | Andy Zhou           | arxiv       | Jan-24 | https://arxiv.org/abs/2401.17263         | https://github.com/lapisrocks/rpo                            |
| SPML: A DSL for Defending Language Models Against Prompt Attacks | Reshabh K Sharma    | arxiv       | Feb-24 | https://arxiv.org/abs/2402.11755         | https://prompt-compiler.github.io/SPML/                      |
| LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper | Daoyuan Wu          | arxiv       | Feb-24 | https://arxiv.org/abs/2402.15727         | null                                                         |
| SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding | Zhangchen Xu        | ACL         | Feb-24 | https://arxiv.org/abs/2402.08983         | https://github.com/uw-nsl/SafeDecoding                       |
| GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis | Yueqi Xie           | ACL         | Feb-24 | https://arxiv.org/abs/2402.13494         | https://github.com/xyq7/GradSafe                             |
| Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing | Jiabao Ji           | arxiv       | Feb-24 | https://arxiv.org/abs/2402.16192         | https://github.com/UCSB-NLP-Chang/SemanticSmooth             |
| Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning | Adib Hasan          | arxiv       | Jan-24 | https://arxiv.org/abs/2401.10862         | null                                                         |
| Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement | Heegyu Kim          | arxiv       | Feb-24 | https://arxiv.org/abs/2402.15180         | null                                                         |
| Protecting Your LLMs with Information Bottleneck             | Zichuan Liu         | arxiv       | May-24 | https://arxiv.org/abs/2404.13968         | https://github.com/zichuan-liu/IB4LLMs                       |
| Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge | Weikai Lu           | arxiv       | Apr-24 | https://arxiv.org/abs/2404.05880         | https://github.com/ZeroNLP/Eraser                            |
| RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content | Zhuowen Yuan        | arxiv       | Mar-24 | https://arxiv.org/abs/2403.13031         | null                                                         |
| Detoxifying Large Language Models via Knowledge Editing      | Mengru Wang         | ACL         | Mar-24 | https://arxiv.org/abs/2403.14472         | https://github.com/zjunlp/EasyEdit                           |
| AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks | Yifan Zeng          | arxiv       | Mar-24 | https://arxiv.org/abs/2403.04783         | https://github.com/XHMY/AutoDefense                          |
| Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes | Xiaomeng Hu         | arxiv       | Mar-24 | https://arxiv.org/abs/2403.00867         | https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense |
| Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs | Fan Liu             | arxiv       | Jun-24 | https://arxiv.org/abs/2406.06622         | null                                                         |
| WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models | Liwei Jiang         | arxiv       | Jun-24 | https://arxiv.org/abs/2406.18510         | null                                                         |
| WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs | Seungju Han         | arxiv       | Jun-24 | https://arxiv.org/abs/2406.18495         | null                                                         |
| SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner | Xunguang Wang       | arxiv       | Jun-24 | https://arxiv.org/abs/2406.05498         | null                                                         |
| Merging Improves Self-Critique Against Jailbreak Attacks     | Victor Gallego      | arxiv       | Jun-24 | https://arxiv.org/abs/2406.07188         | https://github.com/vicgalle/merging-self-critique-jailbreaks |
| Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks | Chen Xiong          | arxiv       | May-24 | https://arxiv.org/abs/2405.20099         | null                                                         |
| Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment | Jiongxiao Wang      | arxiv       | Feb-24 | https://arxiv.org/abs/2402.14968         | https://jayfeather1024.github.io/Finetuning-Jailbreak-Defense/ |
| Improving Alignment and Robustness with Circuit Breakers     | Andy Zou            | arxiv       | Jun-24 | https://arxiv.org/abs/2406.04313         | https://github.com/blackswan-ai/circuit-breakers             |
| Robustifying Safety-Aligned Large Language Models through Clean Data Curation | Xiaoqun Liu         | arxiv       | May-24 | https://arxiv.org/abs/2405.19358         | null                                                         |
| Efficient Adversarial Training in LLMs with Continuous Attacks | Sophie Xhonneux     | arxiv       | May-24 | https://arxiv.org/abs/2405.15589         | https://github.com/sophie-xhonneux/Continuous-AdvTrain       |
| SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance | Caishuang Huang     | arxiv       | Jun-24 | https://arxiv.org/abs/2406.18118         | null                                                         |
| Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing | Wei Zhao            | arxiv       | May-24 | https://arxiv.org/abs/2405.18166         | https://github.com/ledllm/ledllm                             |
| Cross-Task Defense: Instruction-Tuning LLMs for Content Safety | Yu Fu               | NAACL       | May-24 | https://arxiv.org/abs/2405.15202         | https://github.com/FYYFU/safety-defense                      |

### Benchmark

| Title                                                        | Author           | Publish | Year   | Link                             | Source Code                                                  |
| ------------------------------------------------------------ | ---------------- | ------- | ------ | -------------------------------- | ------------------------------------------------------------ |
| Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs     | Zhao Xu          | arxiv   | 2024   | https://arxiv.org/abs/2406.09324 | https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking |
| BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards | Diego Dorn       | arxiv   | 2024   | https://arxiv.org/abs/2406.01364 | null                                                         |
| JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Patrick Chao     | arxiv   | 2024   | https://arxiv.org/abs/2404.01318 | https://github.com/JailbreakBench/jailbreakbench             |
| HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal | Mantas Mazeika   | arxiv   | 2024   | https://arxiv.org/abs/2402.04249 | https://github.com/centerforaisafety/HarmBench               |
| SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese | Liang Xu         | arxiv   | 2024   | https://arxiv.org/abs/2310.05818 | https://www.cluebenchmarks.com/                              |
| Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models | Huachuan Qiu     | arxiv   | 2024   | https://arxiv.org/abs/2307.08487 | https://github.com/qiuhuachuan/latent-jailbreak              |
| XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models | Paul RÃ¶ttger     | NAACL   | Aug-23 | https://arxiv.org/abs/2308.01263 | null                                                         |
| Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models | Huachuan Qiu     | arxiv   | Jul-23 | https://arxiv.org/abs/2307.08487 | https://github.com/qiuhuachuan/latent-jailbreak              |
| SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models in Chinese | Liang Xu         | arxiv   | Oct-23 | https://arxiv.org/abs/2310.05818 | https://www.cluebenchmarks.com/                              |
| AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models | Dong shu         | arxiv   | Jan-24 | https://arxiv.org/abs/2401.09002 | null                                                         |
| How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries | Somnath Banerjee | arxiv   | Feb-24 | https://arxiv.org/abs/2402.15302 | https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA |
| AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts | Shaona Ghosh     | arxiv   | Apr-24 | https://arxiv.org/abs/2404.05993 |                                                              |
| Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs     | Zhao Xu          | arxiv   | Jun-24 | https://arxiv.org/abs/2406.09324 | null                                                         |
| BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards | Diego Dorn       | arxiv   | Jun-24 | https://arxiv.org/abs/2406.01364 | null                                                         |
| Improved Generation of Adversarial Examples Against Safety-aligned LLMs | Qizhang Li       | arxiv   | May-24 | https://arxiv.org/abs/2405.20778 | null                                                         |
| JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks | Weidi Luo        | arxiv   | Apr-24 | https://arxiv.org/abs/2404.03027 | https://github.com/EddyLuo1232/JailBreakV_28K                |
| JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Patrick Chao     | arxiv   | Mar-24 | https://arxiv.org/abs/2404.01318 | https://github.com/JailbreakBench/jailbreakbench             |
| JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models | Delong Ran       | arxiv   | Jun-24 | https://arxiv.org/abs/2406.09321 | https://github.com/ThuCCSLab/JailbreakEval                   |

### Survey

| Title                                                        | Author                 | Publish | Year   | Link                             | Source Code                                           |
| ------------------------------------------------------------ | ---------------------- | ------- | ------ | -------------------------------- | ----------------------------------------------------- |
| Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey | Shang Wang             | arxiv   | 2024   | https://arxiv.org/abs/2406.07973 | [null](https://github.com/EddyLuo1232/JailBreakV_28K) |
| Exploring Vulnerabilities and Protections in Large Language Models: A Survey | Frank Weizhen Liu      | arxiv   | 2024   | https://arxiv.org/abs/2406.00240 | [null](https://github.com/EddyLuo1232/JailBreakV_28K) |
| Safeguarding Large Language Models: A Survey                 | Yi Dong                | arxiv   | 2024   | https://arxiv.org/abs/2406.02622 | null                                                  |
| Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression | Junyuan Hong           | arxiv   | 2024   | https://arxiv.org/abs/2403.15447 | null                                                  |
| Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices | Sara Abdali            | arxiv   | 2024   | https://arxiv.org/abs/2403.12503 | null                                                  |
| Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models | Arijit Ghosh Chowdhury | arxiv   | 2024   | https://arxiv.org/abs/2403.04786 | null                                                  |
| Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey | Zhichen Dong           | arxiv   | 2024   | https://arxiv.org/abs/2402.09283 | null                                                  |
| Security and Privacy Challenges of Large Language Models: A Survey | Badhan Chandra Das     | arxiv   | 2024   | https://arxiv.org/abs/2402.00888 | null                                                  |
| Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems | Tianyu Cui             | arxiv   | 2024   | https://arxiv.org/abs/2401.05778 | null                                                  |
| TrustLLM: Trustworthiness in Large Language Models           | Lichao Sun             | arxiv   | 2024   | https://arxiv.org/abs/2401.05561 | null                                                  |
| A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly | Yifan Yao              | arxiv   | 2024   | https://arxiv.org/abs/2312.02003 | null                                                  |
| A Comprehensive Overview of Large Language Models            | Humza Naveed           | arxiv   | 2024   | https://arxiv.org/abs/2307.06435 | null                                                  |
| Safety Assessment of Chinese Large Language Models           | Hao Sun                | arxiv   | 2024   | https://arxiv.org/abs/2304.10436 | null                                                  |
| Holistic Evaluation of Language Models                       | Percy Liang            | arxiv   | 2024   | https://arxiv.org/abs/2211.09110 | null                                                  |
| Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks | Erfan Shayegani        | ACL     | Oct-23 | https://arxiv.org/abs/2310.10844 | https://llm-vulnerability.github.io/                  |
| A Comprehensive Survey of Attack Techniques, Implementation, and Mitigation Strategies in Large Language Models | Aysan Esmradi          | UbiSec  | Dec-23 | https://arxiv.org/abs/2312.10982 | null                                                  |
| Against The Achilles' Heel: A Survey on Red Teaming for Generative Models | Lizhi Lin              | arxiv   | Mar-24 | https://arxiv.org/abs/2404.00629 | null                                                  |

### Others

| Title                                                        | Author           | Publish | Year   | Link                             | Source Code                                   |
| ------------------------------------------------------------ | ---------------- | ------- | ------ | -------------------------------- | --------------------------------------------- |
| "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models | Xinyue Shen      | CCS     | Aug-23 | https://arxiv.org/abs/2308.03825 | https://github.com/verazuo/jailbreak_llms     |
| Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study | Yi Liu           | arxiv   | May-23 | https://arxiv.org/abs/2305.13860 | null                                          |
| Comprehensive Assessment of Jailbreak Attacks Against LLMs   | Junjie Chu       | arxiv   | Feb-24 | https://arxiv.org/abs/2402.05668 | null                                          |
| Summon a Demon and Bind it: A Grounded Theory of LLM Red Teaming in the Wild | Nanna Inie       | arxiv   | Nov-23 | https://arxiv.org/abs/2311.06237 | null                                          |
| A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models | Zihao Xu         | ACL     | Feb-24 | https://arxiv.org/abs/2402.13457 | null                                          |
| Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models | Rima Hazra       | ACL     | Jan-24 | https://arxiv.org/abs/2401.10647 | null                                          |
| Is the System Message Really Important to Jailbreaks in Large Language Models? | Xiaotian Zou     | arxiv   | Feb-24 | https://arxiv.org/abs/2402.14857 | null                                          |
| Testing the Limits of Jailbreaking Defenses with the Purple Problem | Taeyoun Kim      | arxiv   | Mar-24 | https://arxiv.org/abs/2403.14725 | null                                          |
| JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models | Yingchaojie Feng | arxiv   | Apr-24 | https://arxiv.org/abs/2404.08793 |                                               |
| Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs | Javier Rando     | arxiv   | Apr-24 | https://arxiv.org/abs/2404.14461 | null                                          |
| Universal Adversarial Triggers Are Not Universal             | Nicholas Meade   | arxiv   | Apr-24 | https://arxiv.org/abs/2404.16020 | null                                          |
| How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States | Zhenhong Zhou    | arxiv   | Jun-24 | https://arxiv.org/abs/2406.05644 | https://github.com/ydyjya/LLM-IHS-Explanation |
| Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models | Sarah Ball       | arxiv   | Jun-24 | https://arxiv.org/abs/2406.09289 | null                                          |
| Badllama 3: removing safety finetuning from Llama 3 in minutes | Dmitrii Volkov   | arxiv   | Jul-24 | https://arxiv.org/abs/2407.01376 | null                                          |
| "Not Aligned" is Not "Malicious": Being Careful about Hallucinations of Large Language Models' Jailbreak | Lingrui Mei      | arxiv   | Jun-24 | https://arxiv.org/abs/2406.11668 | null                                          |
| Rethinking How to Evaluate Language Model Jailbreak          | Hongyu Cai       | arxiv   | Apr-24 | https://arxiv.org/abs/2404.06407 |                                               |
| Jailbreak Paradox: The Achilles' Heel of LLMs                | Abhinav Rao      | arxiv   | Jun-24 | https://arxiv.org/abs/2406.12702 | null                                          |
| Hacc-Man: An Arcade Game for Jailbreaking LLMs               | Matheus Valentim | arxiv   | May-24 | https://arxiv.org/abs/2405.15902 | null                                          |
